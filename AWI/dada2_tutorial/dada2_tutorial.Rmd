---
title: 'dada 2 tutorial '
output:
  pdf_document: default
  html_document:
    df_print: paged
---

DADA2 is a R package that implements the full amplicon workflow, including filtering, dereplication, chimera identification, and merging paired-end-reads, which introduces a model based approach for correcting amplicon errors without constructing OTUs.

In this Rstudio markdown (version 2.1), a workflow of the dada2 pipeline is explained.
The version of R used throughout this tutorial is 3.6.0.
The version of other program tools used is listed below.
```
dada2  -- version 1.12.1
phyloseq -- version 1.28.0
```
Tree criteria that data needs to meet before begin the workflow
        1.Demultiplexed samples (splitting into individual per sample fastq files)
        2.Non-biological nucleotides have been removed such as primers, adapters
        3.If paired-end sequencing data, the forward and reverse fastq files contain reads in matched order.
        
        
(Commented section is optional command)Initially, the necessary packages must be loded onto the console. 

```{r}
# library("knitr")
# library("BiocStyle")
# .cran_packages <- c("ggplot2", "gridExtra")
# .bioc_packages <- c("dada2", "phyloseq", "DECIPHER", "phangorn")
# .inst <- .cran_packages %in% installed.packages()
# if(any(!.inst)) {
#   install.packages(.cran_packages[!.inst])
# }
# .inst <- .bioc_packages %in% installed.packages()
# if(any(!.inst)) {
#   source("http://bioconductor.org/biocLite.R")
#   biocLite(.bioc_packages[!.inst], ask = F)
# }
# # Load packages into session, and print package version
# sapply(c(.cran_packages, .bioc_packages), require, character.only = TRUE)

library(dada2)
library(phyloseq)
```


Step 1 -This script below lists all the example fastq files from the illumina Miseq sequencing. Example data set can be downloaded from http://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip. This step contains assignment of the path to the data file. 

```{r}
path <- "~/dada2/dada2_data/MiSeq_SOP" # change this directory to where fastq files are stored. 
list.files(path)
```

Step 2 -String manipulation-

String manipulation to get matched lists of the forward and reverse fastq files. 
```{r}
# The format of forward and reverse fastq file name is SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.name = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

Step 3 -Visualization of the quality profiles of the forward reads. 

```{r}
plotQualityProfile(fnFs[1:2])
```
In the graph generated by the script above, frequency of each quality score at each base position is indicated. The median quality score at each position is indicated by the green line. The orange line indicates the quartiles of the quality score distribution. The red line shows the scaled proportion of reads that extend to at least that position

Total sum of quality score of the reads is indicated in red. 
From the graph, truncate of the read is determined at 240 position where the quality score declines.

Step 4 -Visualization of the quality profiles of the reverse reads

```{r}
plotQualityProfile(fnRs[1:2])
```

It is common for reverse reads to have bad quality at the end of the cycle. Trucate will take place at position 160 where the quality distribution crashes. 


Step 5 - Assignment of the filenames after the fastq files are filtered.
```{r}

# Place filtered files in filtered subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names

```

step 6- filtering and trimming

filterAndTrim command is used

Aguments are explained below

truncQ = default value is 2. It truncate reads at the first instance of a quality score less than or equal to truncQ

truncLen = Position of readswhere it is being truncated from

compress = the output file is saved in gzip extention

maxEE = reads with higher than max expected erroes will be discarded

rm.phix = if TRUE, discard reads that match against the ohiX genome as determined by isPhix

```{r}
out <- filterAndTrim(
  fwd = fnFs,
  filt = filtFs, 
  rev = fnRs, 
  filt.rev = filtRs,
  truncLen=c(240,160),
  maxN=0,
  maxEE=c(1,1),
  truncQ=2,
  rm.phix=TRUE,
  compress=TRUE,
  multithread=TRUE)
head(out)
```
The Q score indicated the probability that the base call is incorrect. Q=2 means that the error probability is 63$, so the machine is more likely to be wrong than right.


Step 6.2 - Dereplication -
Dereplication combines all identical sequencing reads into "unique sequences" with a corresponfing "abundance": the number of reads with that unique sequence. Depreplication substantially reduces computation time by eliminating redundant comparisons.

```{r}
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)

# Name the derep-class object by the sample names. 
names(derepFs) <- sample.names
names(derepRs) <- sample.names
```

step 7 - learn the error rates 

Every amplicon dataset has a different set of error rates. 
Blackline shows the estimated error rates after convergence of the machine-learning algorithm.
Redline shows the error rates expected under the nominal definition of the Q-score.
Black lines are a good fit fot the observed rates and the errror rates drop with increased quality as expected. 

```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE) 
```

step 8 - Sample inference
This dada function removes all sequencing errors to reveal the members of the sequenced community. 
```{r}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE, pool="pseudo")
dadaRs <- dada(filtRs, err=errR, multithread=TRUE, pool="pseudo")
dadaFs[[1]]
```

step 9 - Merging forward and reverse reads together -

Merging means combining the forward and reverse reads together to obtain the full denoised sequences (contig ). 
```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
head(mergers[[1]])
```

step 10 - construct an amplicon sequence variant table, which is a higher resolution version of the OTU table produced by traditional methods-

```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```

This yable contains 222 ASVs, and the lengths of the merged sequences all fall within the expected range for this V4 amplicon. 

There are 20 samples in total, 282 ASV were identified.
The rows corresponding to the samples, and columns corresponding to the sequence variants. This table contains 293 ASVs, and the lengths of our merged sequences all fall within the expected range for this V4 amplicon. 


Chimeras are sequences formed fro two or more biological sequences joined together. It is common in amplicon sequencing. 
Step 11 - Removing chimeras- 

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)
```

60chimeric sequences were discarded
Abundances of those variants we see they account for only about 4% of thje merged sequences reads. 

Step 12 -Track reads through the pipeline-

```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# if processing a single sample, remove the sapply calls
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

This is the summary of sequences through the whole previous steps. 
Most of the raw reads remained filtered. 


Step 13 -Assign taxonomy - the general fasta release files can be downloaded from https://unite.ut.ee/repository.php

```{r}
taxa <- assignTaxonomy(
  seq = seqtab.nochim, 
  refFasta = "~/dada2/dada2_tutorial/silva_nr_v132_train_set.fa.gz", multithread=TRUE)
taxa.print <- taxa # Removing sequence rownames for display only 
rownames(taxa.print) <- NULL
head(taxa.print)
```

step 14 - evaluation of accuracy (optional step)-

 ```{r}
# unqs.mock <- seqtab.nochim["Mock",]
# unqs.mock <- sort(unqs.mock[unqs.mock>0], decreasing=TRUE) # Drop ASVs absent in the Mock
# cat("DADA2 inferred", length(unqs.mock), "sample sequences present in the Mock community. \n")
#
# mock.ref <- getSequences(file.path(path, "HMP_MOCK.v35.fasta"))
# match.ref <- sum(sapply(names(unqs.mock), function(x) any(grepl(x, mock.ref))))
# cat("0f those,", sum(match.ref), "were exact matches to the expected reference sequences. \n")
```


STEP 15 - exporting the OTU table - (IMPORTANT)

This step is to demonstrate how to export the tables produced by the DADA2 pipeline out to your local folder.

```{r}
# Giving our seq headers more manageable names
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")

for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, seq="_")
}

# making and writing out a fast of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
# File name can be changed by modifying "ASVs.fa"
write(asv_fasta, "ASVs_example.fa") 


# ASV table
asv_tab <- t(seqtab.nochim)
row.names(asv_tab) <- sub(">", "", asv_headers)
# File name can be changed by modifying "ASVs_counts.tsv"
write.table(asv_tab, "ASVs_example.tsv", sep="\t", quote=F, col.names=NA)

# tax table (This is commented out due to memory allocation failure on my laptop)
# asv_tax <- taxa
# row.names(asv_tax) <- sub(">", "", asv_headers)
# write.table(asv_tax, "ASVs_taxoomy.tsv", sep="\t", quote=F, col,names=NA)

```



=========================================================================
========================================================================
========================================================================
              Steps below this point are optional!!
=======================================================================
======================================================================
=======================================================================


# ```{r}
# samples.out <- rownames(seqtab.nochim)
# subject <- sapply(strsplit(samples.out, "D"), `[`, 1)
# gender <- substr(subject,1,1)
# subject <- substr(subject,2,999)
# day <- as.integer(sapply(strsplit(samples.out, "D"), `[`, 2))
# samdf <- data.frame(Subject=subject, Gender=gender, Day=day)
# samdf$When <- "Early"
# samdf$When[samdf$Day>100] <- "Late"
# rownames(samdf) <- samples.out
# 
# ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE),
#                sample_data(samdf))
#                #tax_table(taxa))
# ps <- prune_samples(sample_names(ps) != "Mock", ps)
```


```{r}
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste("ASV", seq(ntaxa(ps)))
ps
```

```{r}
new.names <- paste0("ASV1", seq(ntaxa(ps))) # Define new names ASV1, ASV2, ...
seqs <- taxa_names(ps) # Store sequences
names(seqs) <- new.names # Make map from ASV1 to full sequence
taxa_names(ps) <- new.names # Rename to human-friendly format
```

Shannon - richness of the community
Simpson - evenness of the community
```{r}
plot_richness(ps, x="Day", measures=c("Shannon", "Simpson"), color="When")
```

```{r}
ord.nmds.bray <- ordinate(ps, method="NMDS", distance="bray")
plot_ordination(ps, ord.nmds.bray, color="when", title="bray NMDS")
```

